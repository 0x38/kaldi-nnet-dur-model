!obj:pylearn2.train.Train {
  dataset : &train !pkl: "${TRAIN_PKL}",
  
  model : !obj:pylearn2.models.mlp.MLP {
    layers: [
            !obj:pylearn2.models.maxout.Maxout {
                        layer_name: 'h0',
                        num_units: ${H0_DIM},
                        num_pieces: 5,
                        irange: .05,
                        # Rather than using weight decay, we constrain the norms of the weight vectors
                        max_col_norm: 5.
             },                  
        
        
        
      !obj:pylearn2.models.maxout.Maxout {
                  layer_name: 'h1',
                  num_units: ${H1_DIM},
                  num_pieces: 5,
                  irange: .05,
                  # Rather than using weight decay, we constrain the norms of the weight vectors
                  max_col_norm: 2.
       },      


      !obj:durmodel_elements.LogNormalMixtureLogLikelihood {
        num_mixtures : 2,
        layer_name: 'out',
        irange: 0.,
        #W_lr_scale: 2.0,
        #imax_col_norm: 1.0,
        init_bias : 0.5
      }

    ],
    
    nvis: ${INPUT_DIM},
    
    #input_space: !obj:pylearn2.space.CompositeSpace {
      #components: [
        #!obj:pylearn2.space.VectorSpace {
          #dim: ${INPUT_DIM}, 
        #},
        #!obj:pylearn2.space.IndexSpace {
          #dim: 1, 
          #max_labels: ${NUM_SPEAKERS}
        #}
      #]
    #},
    #input_source: ['features', 'speakers']
  },
  

  algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
    # The learning rate determines how big of steps the learning algorithm
    # takes.  Here we use fairly big steps initially because we have a
    # learning rate adjustment scheme that will scale them down if
    # necessary.
    learning_rate: 0.01,

    learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
             init_momentum: 0.5
    },
    
    
    
    #cost: !obj:pylearn2.costs.mlp.dropout.Dropout {
             #input_include_probs: { 'h0' : 1.0 },
             #input_scales: { 'h0': 1.0 }
    #},    

    # Each gradient step will be based on this many examples
    batch_size : 200,
    
    monitoring_batches : 100,
    
    monitoring_dataset: {
      train: *train,
      valid: !pkl: "${DEV_PKL}",
    },
 

    # We'll use the monitoring dataset to figure out when to stop training.
    #
    # In this case, we stop if there is less than a 1% decrease in the
    # training error in the last epoch.  You'll notice that the learned
    # features are a bit noisy. If you'd like nice smooth features you can
    # make this criterion stricter so that the model will train for longer.
    # (setting N to 10 should make the weights prettier, but will make it
    # run a lot longer)

    #termination_criterion : !obj:pylearn2.termination_criteria.MonitorBased {
    #    prop_decrease : 0.01,
    #    N : 1,
    #}
    termination_criterion : !obj:pylearn2.termination_criteria.MonitorBased {
        channel_name: "valid_out_ppl",
        prop_decrease: 0.,
        N: 5
    },
    #update_callbacks: !obj:pylearn2.training_algorithms.sgd.ExponentialDecay {
    #    decay_factor: 1.00001,
    #    min_lr: .001
    #}
    
  },
  
  extensions: [
    #!obj:pylearn2.training_algorithms.sgd.LinearDecayOverEpoch {
        #start: 5,
        #saturate: 20,
        #decay_factor: 0.5
    #},
    !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
       channel_name: "valid_out_ppl",
       save_path: "${MODEL_SAVE_PATH}"
    },

  ],
  
}
